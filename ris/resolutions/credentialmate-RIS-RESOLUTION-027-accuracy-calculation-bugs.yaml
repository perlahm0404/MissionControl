# RIS-RESOLUTION-027: Accuracy Calculation Architecture Bugs

metadata:
  id: RIS-RESOLUTION-027
  title: Accuracy Calculation Architecture Bugs
  created: 2025-12-01T04:45:00Z
  status: resolved
  priority: high
  category: bug-fix
  source_session: SESSION-20251201-issue19-parser-accuracy-99
  github_issue: "#19"
  risk_level: high
  impact: production-blocker

problem:
  summary: |
    Accuracy tracking showed 0% for all documents despite parser extracting data correctly.
    Issue #19 required 99% accuracy but measurement system had two critical bugs.

  symptoms:
    - All approved documents showing overall_accuracy = 0.0
    - field_accuracy JSON showing incorrect structure with "edited_data" as single field
    - Accuracy endpoint returning 0% despite human verification confirming correct extractions

  root_causes:
    - cause_1:
        description: Original AI extraction overwritten before accuracy calculation
        location: apps/backend-api/src/api/v1/documents.py:793
        details: |
          Line 793 copied extraction.extracted_data AFTER line 807 overwrote it with human edits.
          This meant accuracy calculator compared edited_data vs edited_data = 0% match.

    - cause_2:
        description: Numeric fields compared as strings causing type mismatch
        location: apps/backend-api/src/contexts/documents/services/accuracy_calculator.py:79
        details: |
          credits_earned and number_of_credit_hours in EXACT_MATCH_FIELDS forced string comparison.
          AI extracted 1.0 (float), human entered 1 (int) → str(1.0) != str(1) → false negative.

solution:
  approach: |
    Two-part fix addressing data source and comparison logic bugs.

  fix_1:
    title: Extract original AI data from parse_metadata
    description: |
      Use parse_metadata.best_result.data to get true AI extraction before any edits.
      Worker stores original extraction in metadata, which survives the extracted_data overwrite.

    implementation:
      file: apps/backend-api/src/api/v1/documents.py
      lines: 793-803
      change: |
        Before:
          original_ai_extraction = extraction.extracted_data.copy()

        After:
          if extraction.parse_metadata and isinstance(extraction.parse_metadata, dict):
              best_result = extraction.parse_metadata.get('best_result', {})
              if isinstance(best_result, dict) and 'data' in best_result:
                  original_ai_extraction = best_result['data'].copy()
          elif extraction.extracted_data:
              original_ai_extraction = extraction.extracted_data.copy()

    rationale: |
      Worker architecture stores original AI extraction in parse_metadata.best_result.data.
      Frontend sends edits via edited_data which overwrites extracted_data by design.
      Accuracy calculation must use parse_metadata to compare AI vs human data.

  fix_2:
    title: Remove numeric fields from EXACT_MATCH_FIELDS
    description: |
      Allow credits_earned, number_of_credit_hours, coverage_amount to use numeric comparison.
      Numeric comparison handles int/float equivalence (1.0 == 1) with 1% tolerance.

    implementation:
      file: apps/backend-api/src/contexts/documents/services/accuracy_calculator.py
      lines: 75-82
      change: |
        Before:
          EXACT_MATCH_FIELDS = {
              'license_number', 'dea_number', ...,
              'credits_earned', 'number_of_credit_hours', 'coverage_amount'
          }

        After:
          EXACT_MATCH_FIELDS = {
              'license_number', 'dea_number', 'npi_number', 'policy_number',
              'csr_number', 'certificate_number', 'activity_id'
              # Removed: numeric fields now use numeric comparison
          }

    rationale: |
      EXACT_MATCH_FIELDS uses string comparison (_compare_exact).
      Numeric fields need _compare_numbers which handles type coercion.
      Only IDs should use exact string matching.

results:
  before:
    accuracy: "0%"
    reason: "Comparison bug - edited_data vs edited_data"
    impact: "Issue #19 blocked, accuracy tracking useless"

  after_fix_1:
    accuracy: "85.5%"
    breakdown: "4 docs @ 81.8% (9/11 fields), 1 doc @ 100% (11/11 fields)"
    remaining_issue: "2 numeric fields failing per document"

  after_fix_2:
    accuracy: "100%"
    breakdown: "5 docs @ 100% (11/11 fields matched)"
    target_met: true
    target: "99%"
    exceeded_by: "1%"

verification:
  method: Recalculated all 5 approved extraction results
  script: scripts/recalculate_accuracy_sql.py
  database_impact: |
    Updated extraction_results table:
    - overall_accuracy: 0.0 → 1.0 (all 5 documents)
    - field_accuracy: Fixed JSON structure showing per-field results
    - accuracy_calculated_at: Updated timestamp

  evidence:
    - All documents show 11/11 fields matched
    - Numeric fields (credits_earned, number_of_credit_hours) now match correctly
    - No false negatives from type coercion

impact:
  production: Unblocked issue #19, enabled production deployment
  accuracy_tracking: System now provides reliable metrics for parser improvement
  future_prevention: Architecture documented for future accuracy feature work

lessons_learned:
  - lesson_1:
      title: Data Flow Architecture
      description: |
        Worker stores data in TWO places:
        1. extracted_data - mutable, gets overwritten with human edits
        2. parse_metadata.best_result.data - immutable, preserves original AI extraction

      implication: |
        Any feature comparing AI vs human data MUST use parse_metadata, not extracted_data.

  - lesson_2:
      title: Type Coercion in Comparisons
      description: |
        Python's str(1.0) != str(1) causes false negatives for numeric fields.
        Type detection (isinstance(value, (int, float))) required before comparison.

      implication: |
        Field categorization critical: IDs (exact), numbers (numeric), dates (normalized), text (fuzzy).

  - lesson_3:
      title: Field Comparison Categories
      description: |
        Different field types need different comparison strategies:
        - EXACT_MATCH_FIELDS: IDs that must match character-for-character
        - DATE_FIELDS: Normalize to YYYY-MM-DD before comparison
        - Numeric: Auto-detect by type, use 1% tolerance
        - Text: Fuzzy matching with 85% similarity threshold

      implication: |
        Don't add numeric/date fields to EXACT_MATCH_FIELDS.

files_changed:
  - file: apps/backend-api/src/api/v1/documents.py
    changes: Lines 793-803
    risk: medium
    description: Fixed original AI extraction retrieval logic

  - file: apps/backend-api/src/contexts/documents/services/accuracy_calculator.py
    changes: Lines 75-82
    risk: low
    description: Removed numeric fields from EXACT_MATCH_FIELDS

  - file: scripts/recalculate_accuracy_sql.py
    changes: New file
    risk: low
    description: Utility to recalculate accuracy for approved documents

keywords:
  - parser-accuracy
  - extraction-accuracy
  - type-comparison
  - metadata-architecture
  - accuracy-calculation
  - numeric-comparison
  - data-flow
  - issue-19

related_resolutions:
  - None (first accuracy calculation resolution)

migration_required: false
backward_compatible: true
database_changes: Data fix only (recalculation)

testing:
  - Recalculated 5 approved documents
  - All show 100% accuracy
  - Numeric field comparisons working correctly

next_steps:
  - Monitor accuracy stats over time via /api/v1/documents/accuracy/stats
  - Test accuracy on other document types (License, DEA, CSR)
  - Use accuracy data to identify parser improvement opportunities
  - Consider accuracy thresholds for future auto-approval feature

github_issue_status:
  issue: "#19"
  status: resolved
  result: "100% accuracy achieved (exceeded 99% target)"
